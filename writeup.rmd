---
title: "523 Final Project - Indeed Job Search"
author: "Yumemechi Fujita, Yen Hua Cheng, Eden Huang, Faustine Li, Shoaji Li"
output: html_document
---

<br><br>

## Introduction

## Webscraping Indeed

We utilized the `jobSearch` function in the `jobR` package to scrap information from "indeed.com". The function took several arguments: publisher, query, country, start, location, and limit and generated a data frame that contains information about jobs such as job title, location, description and job links etc. 

One shorage of this function was that we could not specify the number of entries to be generated unless the number is less than 25. Our solution was to scrap multiple times and concatenate the dataframes. For example, if we were to generate 120 jobs, we scrapped 5 times with the first four times generating 25 entries each and the last time generating 20 entries. Then we concatenate the 5 dataframes into a big data frame of 120 rows.

We also add a column "job.descript", which contains the full job descriptions for the jobs. To do this, we read the html links in dataframe created previously and grab the job descriptions under the "#job_summary" node.

## Text cleaning

After we obtained the raw job decriptions from Indeed, we needed to clean the results. We heavily utilized the text-mining package `tm`. It converted our dataframe of job descriptions into a "corpus" object. From there transformations can be done over that corpus object including removing stop words, punctuation, extra white-space, etc. Many of these steps are aggregated in the `clean_corpus` function that we wrote. Stemming the documents is also an important step. Using the `snowballC` package allowed words related by word stem to be condensed.

Finally the cleaned words were used to create a document term matrix (DTM). The idea is that documents are broken into individual words. The rows are each document in the corpus and each column is a word. Each element is the count of that word in that document. Because a single document only has a small subset of words, the matrix is often sparse. Thankfully, the `tm` package has a `dtm` class that stores the document term matrix in a memory-efficient way. 

<br>

## Clustering Jobs with LDA

We were interested in how jobs are related to one another. Because the types of job are not know apriori, clustering jobs is considered a *unsupervised learning* problem. While there are many clustering algorithms that could have been suitable, we chose Latent Dirchlet Allocation because of its previous use in topic modeling. 

LDA is a genertive model. The model assumes that for each document in a corpus, $d_n$.

1. $\theta \sim Dir(\alpha)$

2. For each word in $d_n$:
  
    a. Sample a topic, $z_i \sim Multinomial(\theta)$
  
    b. Sample a word, $w_n \sim p(words|z_i, \beta)$

In other words, the LDA model assumes that a document is a collection of words coming from a mixture of topics. The variables $\alpha$ and $\beta$ are prior parameters for the model. The topic variable is not directly observed, therefore acts as a *latent* variable. 

To assign a document a mixture of topics, we need to know the probability that the words in that document were generated using the model, $p(d_n |\alpha, \beta)$. We can find this value using Baye's Rule, but in general this posterior density is difficult to calculate. However there are many techniques such as Gibbs sampling to estimate the posterior. The final outcome is that each document is a assigned a mixture of topics, 30% from topic 1, 40% from topic 2, and so forth. Although topic assignment is probabalistic, to visualize similar jobs we grouped jobs based on the topic with the highest contribution. 

The R package `topicmodels` implements latent dirchlet allocation. The input to `LDA` is a number of clusters and a document term matrix. The default method for training the model is variational E-M, a technique for posterior inference. The output is a trained model, where each document is has cluster assignments. Training on the roughly 1000 documents we scraped from Indeed took 1-2 minutes. 

<br>

## Shiny App




### Word Cloud

In the word cloud function, we defined some common words that are not related to job skills (ex. one, job, new, will, work etc.) as removed_dictionary. Then, we used `tm_map`, `removeWords` function to remove those unrelated words. After this, we used `wordcloud()` to plot a word cloud. The words were plotted in decreasing frequency in the word cloud. 

### Job Map

The function of job map is to visualize geographically the jobs of each cluster for all states. Users may click the jobs and enter the Indeed.com websites through the urls. Also, the job map researves the google map feature such as zoom in/out, street view and satelite view.

To achieve this, we created a `map_jobs` function, which took a data frame. In our project, this function takes the data frame containing information about the jobs in the *category* and *state* users select in the sidebar. After taking in the job data frame, the `map_jobs` function first generates lattitudes and longitudes for the job location. To do this, we created a data frame called `geo` that contains lattitudes and longitudes for over 25,000 cities in U.S and join the job data frame with it according to the city name. The code that generates the data frame can be found in the "Webscraping for mapping" section in "preprocessing.R".

After that, we started plotting the jobs on our gvisMap. We first filtered out jobs such that the locations could not be matched by the `geo` dataframe. Then it created another column called "tip" to store the urls for the jobs whose locations are available. After that, we plotted the jobs on the `gvisMap` by using the `gvisMap` function in the `ggmap` package.

<br>

## Conclusion


## References

https://en.wikipedia.org/wiki/Document-term_matrix

https://www.cs.princeton.edu/~blei/papers/BleiNgJordan2003.pdf

https://cran.r-project.org/web/packages/topicmodels/vignettes/topicmodels.pdf


